name: Auto-Download Video v2

on:
  workflow_dispatch:
  # schedule: # use cron-job.org instead
  #   - cron: '35 * * * *'

permissions:
  contents: write

concurrency:
  group: Auto-Download Video
  cancel-in-progress: false

jobs:
  check-feed:
    name: Check Feed for Valid Entries
    runs-on: ubuntu-latest
    outputs:
      should_process: ${{ steps.feed-parser.outputs.should_process }}
      urls: ${{ steps.feed-parser.outputs.urls }}
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install parsing dependencies
        run: pip install feedparser

      - name: Parse and Validate Feed
        id: feed-parser
        run: |
          import feedparser
          from datetime import datetime, timedelta, timezone
          import os
          import json

          def alt_link(entry):
              for link in entry.get('links', []):
                  if link.get('rel') == 'alternate':
                      return link['href']
              return entry.get('link', '')

          feed_url = "https://raw.githubusercontent.com/braboobssiere/holedex-song-list/main/feeds/holodex.atom" 
          # holodex
          feed = feedparser.parse(feed_url)

          current_time = datetime.now(timezone.utc)
          time_threshold = current_time - timedelta(minutes=61)
          matched_entries = []

          for entry in feed.entries:
              updated_str = entry.get('updated')
              if not updated_str:
                  continue
                  
              try:
                  entry_time = datetime.fromisoformat(updated_str).astimezone(timezone.utc)
              except (ValueError, TypeError):
                  continue
              
              title = entry.get('title', '')

              if 'unarchive' not in title.lower() and 'アーカイブなし' not in title:
                  continue

              if time_threshold <= entry_time <= current_time:
                  link = alt_link(entry)
                  if link:
                      matched_entries.append((entry_time.isoformat(), link))

          # Sort by most recent first
          matched_entries.sort(reverse=True, key=lambda x: x[0])
          entries = [{"url": item[1]} for item in matched_entries[:4]]

          output_path = os.environ['GITHUB_OUTPUT']
          with open(output_path, 'a') as f:
              f.write(f'should_process={bool(entries)}\n')
              f.write(f'urls={json.dumps(entries)}\n')
        shell: python

  process-video:
    name: Process and Upload Videos
    needs: check-feed
    if: ${{ needs.check-feed.outputs.should_process == 'true' }}
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Setup Dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y ffmpeg wireguard-tools jq curl zip parallel
          python3 -m pip install --pre yt-dlp[default,curl-cffi]
          
      - name: VPN Setup 
        env: 
          PRIMARY_CONFIG: ${{ secrets.WIREGUARD_TRADEWAR }}
          SECONDARY_CONFIG: ${{ secrets.WIREGUARD_HARVARD }}
        run: |
          sudo mkdir -p /etc/wireguard/
          WG_CONF="/etc/wireguard/wg0.conf"

          handle_vpn_attempt() {
            local config_content="$1"
            local label="$2"
            
            echo "$config_content" | sudo tee $WG_CONF > /dev/null
            sudo chmod 600 $WG_CONF

            for i in {1..3}; do
              if timeout 60 sudo wg-quick up wg0; then
                return 0
              fi
              echo "${label^} VPN attempt $i/3 failed"
              sleep 3
            done
            return 1
          }

          if ! handle_vpn_attempt "$PRIMARY_CONFIG" "primary"; then
            echo "::warning::Primary failed - Trying secondary"
            if ! handle_vpn_attempt "$SECONDARY_CONFIG" "secondary"; then
              echo "::error::All VPN configurations failed after 3 attempts each"
              exit 11
            fi
          fi

          if ! sudo wg show wg0 >/dev/null 2>&1; then
            echo "::error::VPN connection verification failed"
            exit 12
          fi

      - name: Concurrent Downloads
        continue-on-error: true
        run: |
          mkdir -p output
          URLS=$(echo '${{ needs.check-feed.outputs.urls }}' | jq -r '.[].url')
          
          parallel -j4 \
          'cmd="yt-dlp -i --no-progress -S \"+res:1080\" --retry-sleep 3 \
            --windows-filenames --live-from-start --write-thumbnail \
            --embed-metadata --no-embed-info-json \
            -o \"output/%(title).170B [%(id)s] (%(resolution)s).%(ext)s\" {}";
          # remove ,+vcodec:av01 from -S until youtube fixed it
          echo "Download command: $cmd";
          eval "$cmd"' ::: "${URLS[@]}" >> output/logs.txt 2>&1

      - name: VPN Cleanup
        if: ${{ always() }}
        run: sudo wg-quick down wg0 || true

      - name: Verify output file 
        run: |
          # Count files in output directory (including hidden files)
          FILE_COUNT=$(find output/ -type f | wc -l)
          
          if [ $FILE_COUNT -lt 2 ]; then
            echo "Error: yt-dlp have a problem downloading video (found $FILE_COUNT)"
            exit 13
          fi

      - name: Generate tag and split large files
        id: prep
        run: |
          TIMESTAMP=$(date -u +%Y%m%d-%H%M%S)
          TAG="${TIMESTAMP}"
          echo "TAG=$TAG" >> $GITHUB_OUTPUT          
          cd output
          find . -type f -size +1792M -exec bash -c '
            for file; do
              zip -s 1792m -r "${file}.zip" "${file}"
              rm "${file}"
            done
          ' bash {} +

      - name: Create draft release
        uses: softprops/action-gh-release@v2
        with:
          tag_name: ${{ steps.prep.outputs.TAG }}
          name: "Release ${{ steps.prep.outputs.TAG }}"
          draft: true
          files: |
            output/**

      - name: Cleanup tag
        run: git push --delete origin "${{ steps.prep.outputs.TAG }}" || true

      - name: Upload Files to Cloud Storage
        run: |
          UPLOAD_URL="https://upload.gofile.io/uploadfile"
          guest_token=""
          folder_id=""
          UPLOAD_LINKS=""
          FIRST_FILE=true

          while IFS= read -r -d '' file; do
            echo "▫️ Starting upload: $(basename "$file")"
            extra_args=()
            if [ "$FIRST_FILE" = false ]; then
              extra_args+=(-H "Authorization: Bearer $guest_token" -F "folderId=$folder_id")
            fi
            
            RESPONSE=$(curl -s -X POST -F "file=@\"$file\"" "${extra_args[@]}" "$UPLOAD_URL")

            PYTHON_OUTPUT=$(python3 <<EOF
          import json, sys
          try:
              data = json.loads('''$RESPONSE''')
              if "$FIRST_FILE" == "true":
                  guest_token = data['data']['guestToken']
                  folder_id = data['data']['parentFolder']
                  print(f"{guest_token}\t{folder_id}")
              else:
                  print("-\t-")          
              status = 'ok' if data['status'] == 'ok' else 'error'
              result_data = data.get('data', {}).get('downloadPage', '') if status == 'ok' else data.get('data', '')
              print(f"{status}\t{result_data}")
          except Exception as e:
              print(f"error\tJSON parsing failed: {str(e)}")
              sys.exit(1)
          EOF
            )

            {
              read -r token_part folder_part
              read -r status_type result_data
            } <<< "$PYTHON_OUTPUT"

            if [ "$FIRST_FILE" = true ]; then
              guest_token="$token_part"
              folder_id="$folder_part"
              FIRST_FILE=false
              echo "GUEST_TOKEN=$guest_token" >> $GITHUB_ENV
              echo "FOLDER_ID=$folder_id" >> $GITHUB_ENV
              echo "Created folder ID"
            fi

            if [ "$status_type" != "ok" ]; then
              echo "::error file=$file::Upload failed: $result_data"
            else
              LINK="$result_data"
              UPLOAD_LINKS+="- $(basename "$file")\n"
              echo "Success: $LINK"
            fi
          done < <(find output/ -type f -print0)

          echo "### File Uploads: $LINK" >> $GITHUB_STEP_SUMMARY
          echo -e "\n$UPLOAD_LINKS" >> $GITHUB_STEP_SUMMARY

          curl -X POST \
            -H "Content-Type: application/json" \
            -d "{\"content\":\"File Uploads: $LINK \n$UPLOAD_LINKS\"}" \
            "${{ secrets.DISCORD_WEBHOOK }}"
            
